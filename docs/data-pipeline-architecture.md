# 🔄 데이터 파이프라인 아키텍처
![데이터 파이프라인 아키텍처](images/data-pipeline-architecture.png)

### 데이터 흐름
`API -> S3 -> Glue -> S3 -> Redshift -> Web Service`
1. 원시 데이터는 API를 통해 수집된 후 S3에 원본 그대로 저장
2. 이후 Glue Crawler와 Glue Spark Job을 활용해 데이터를 정제하고 변환하며 결과는 S3에 다시 저장
3. 마지막으로 Redshift에 적재하여 분석과 집계를 수행하고, 최종 결과를 웹 서비스에서 활용

### 레이어별 역할
| **레이어** | **저장 위치** | **데이터 형식**           | **주요 내용**                           |
|------------|---------------|---------------------------|-----------------------------------------|
| **브론즈** | S3            | JSON                      | 원본 데이터 저장 - 정제되지 않은 상태  |
| **실버**   | Redshift      | 정형화 테이블            | 데이터 정제 및 구조화 - 분석 준비 단계 |
| **골드**   | Redshift      | 집계 및 분석된 정형화 테이블 | 집계 및 분석 결과 데이터 - 최종 활용 단계 |